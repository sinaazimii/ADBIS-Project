{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57b3af7-e31d-4a29-a624-f0899acf54ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting up the environment\n",
    "from pyspark import SparkContext\n",
    "from functools import reduce\n",
    "from collections import Counter\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as PySparkFunctions\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from itertools import chain\n",
    "import statistics\n",
    "# sc.stop() #stop previous session\n",
    "sc = SparkContext(\"local\", \"URRDD\") #create an instance of SparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b61c46d-9be6-44c2-bee5-9eac4bc38c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading from stopwrods file\n",
    "f = open(\"/home/jovyan/Data/stopwords_en.txt\",\"r\")\n",
    "global stopwords\n",
    "# split each line\n",
    "stopwords = f.read().splitlines()\n",
    "#  function to find top ten occuring words related to each user and excluding stop words\n",
    "def most_frequent(List,stopwords):\n",
    "    words = []\n",
    "    for word in List:\n",
    "        if word not in stopwords:\n",
    "            if word != '' :\n",
    "                words.append(word)\n",
    "    occurence_count = Counter(words)    \n",
    "    top_ten = occurence_count.most_common(10)\n",
    "    return [x[0] for x in top_ten] #we don't want the count, so returning only the words\n",
    "# definig min and max functions\n",
    "def find_max(x,y):\n",
    "    if x <= y : return y\n",
    "    else: return x\n",
    "def find_min(x,y):\n",
    "    if x <= y : return x\n",
    "    else: return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da011bf-640d-4cb3-afef-04005e7863ad",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exercise 1.2.a\n",
    "lines = sc.textFile(\"/home/jovyan/Data/users_libraries.txt\") #read lines from user_libraries.txt file as RDD\n",
    "user_liked = lines.map(lambda x: (x.split(\";\")[0], x.split(\";\")[1:] )) #create pair by applying a lambda function and map lines to it\n",
    "user_liked.collect() #collecting created pair and checking if RDD's correctly generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd40dbf-a1b8-4b5d-bb01-c57b2ba72c8f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exercise 1.2.b\n",
    "import re\n",
    "lines = sc.textFile(\"/home/jovyan/Data/papers.csv\") #read lines from papers.csv file as RDD\n",
    "# splitting the lines read from csv files by comma. creating a new RDD from that\n",
    "paper_words_inter = lines.map(lambda x: (x.split(\",\")[0], [reduce(lambda i, j: i + j, x.split(',')[13:])][0]))\n",
    "# Creating a RDD which contains tuples of (paper_id,[words_in_it]) spliting by space and \" .\n",
    "paper_words = paper_words_inter.map(lambda x: (x[0],re.split(' |\"',x[1])))\n",
    "paper_words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c014c7d8-1f22-4f01-a77f-46c972bfdfbe",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exercise 1.3\n",
    "\n",
    "# ! ATTENTION !\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# \"for each user the top-10 most frequent words appearing in the papers she\n",
    "# likes. Exclude the stop words listed in stopwords en.txt\" \n",
    "# I assumed the above statement in project document referes to words included in both title and abstarct.\n",
    "# Since it does not clarify which one and my question in the forum regarding that remained unanswerd\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "user_liked_splited = user_liked.map(lambda x: (x[0],x[1][0].split(\",\"))) \n",
    "# creating a new RDD which each user only liked one paper and keys are not unique.in order\n",
    "# to be able to join\n",
    "user_liked_flated = user_liked_splited.flatMapValues(lambda x: x).collect()\n",
    "for i in range(len(user_liked_flated)):\n",
    "    user_liked_flated[i] = (user_liked_flated[i][1],user_liked_flated[i][0]) \n",
    "    # swaping each item key and value for join transformation     \n",
    "#convert to RDD after swaping and join two RDDs\n",
    "user_liked_words = sc.parallelize(user_liked_flated).join(paper_words)\n",
    "#Creating a RDD contains each user and words in each of his/her liked papers. \n",
    "# in this RDD (user_words) we have reaccuring user_ids\n",
    "user_words = user_liked_words.map(lambda x: x[1])\n",
    "# using reduce by key to combine all words related to a user to it's unique ID, \n",
    "# now user_ids are unique in user_combined_words\n",
    "user_combined_words = user_words.reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "# mapping RDD to get a new RDD called user_top_ten which contains (user_id, top_ten_words) \n",
    "user_top_ten = user_combined_words.map(lambda x: (x[0], most_frequent(x[1],stopwords)))\n",
    "user_top_ten.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42add950-d1af-4566-81d8-66555f9d5d58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exercise 1.4.a)\n",
    "number_of_users = user_liked.groupByKey().count()\n",
    "number_of_papers = paper_words.groupByKey().count()\n",
    "# Number of \"distinct\" user_ids(or users by meaning)\n",
    "print(\"Number of distinct users is:\", number_of_users) \n",
    "print(\"Number of distinct papers is:\",number_of_papers) \n",
    "# Number of \"distinct\" paper_ids(or papers by meaning)\n",
    "\n",
    "ratings_rdd = user_liked.map(lambda x: x[1][0].split(\",\")) \n",
    "# spliting each liked paper(since it is a string)\n",
    "number_of_ratings_rdd = ratings_rdd.map(lambda x: len(x)) \n",
    "# we get each t=(user,[liked_papers]) t[1], so now this new rdd contains number of papers each user liked\n",
    "ratings_count = number_of_ratings_rdd.reduce(lambda x,y : x+y) \n",
    "# using reduce action to sum every index of this rdd and printing as result\n",
    "print(\"Number of all ratings is:\",ratings_count)\n",
    "\n",
    "# Exercise 1.4.b)\n",
    "\n",
    "#using the rdd we created earlier which contains number of liked papers \n",
    "#for each user at every index and reduce action to find minimum number, here I used a function find_min()\n",
    "min_ratings = number_of_ratings_rdd.reduce(lambda x,y: find_min(x,y)) \n",
    "print(\"Minimum number of rating a user has given is:\",min_ratings)\n",
    "\n",
    "# Exercise 1.4.c)\n",
    "\n",
    "#using the rdd we created earlier which contains number of liked papers \n",
    "#for each user at every index and reduce action to find maximum number, here I used a function find_max()\n",
    "max_ratings = number_of_ratings_rdd.reduce(lambda x,y: find_max(x,y))\n",
    "print(\"Maximum number of rating a user has given is:\",max_ratings)\n",
    "\n",
    "# Exercise 1.4.d)\n",
    "print(\"Average number of user ratings is:\",ratings_count/number_of_users)\n",
    "\n",
    "# Exercise 1.4.e)\n",
    "number_of_ratings_rdd_list = number_of_ratings_rdd.collect() \n",
    "# now that we have a RDD which each item of it is the number of papers each user liked,\n",
    "# we can easily use python statistics library to find the standard deviation of them.\n",
    "print(statistics.stdev(number_of_ratings_rdd_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6e8f0c-cb57-44ab-8b94-a6d664caee64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# each paper(not unique) and users who rated it\n",
    "paper_users = user_liked_words.map(lambda x: (x[0],x[1][0]))\n",
    "# paper_combined_users contains each paper and a tuple of users who rated it. this time papers are unique\n",
    "# I did this using reduceByKey and adding a \"|\" in between so we can count later\n",
    "paper_combined_users = paper_users.reduceByKey(lambda x,y:x+\"|\"+y)\n",
    "# counting | and adding one since the number of splitter is one less than theb number of users or \n",
    "# by meaning ratings for each paper\n",
    "# paper_number_of_rates contains  number of ratings for each unique paper, (I removed the paper_id since we\n",
    "# don't need it)\n",
    "paper_rates_rdd  = paper_combined_users.map(lambda x: x[1].count(\"|\")+1)\n",
    "# Exercise 1.4.f)\n",
    "#using the rdd we created earlier which contains number of ratings \n",
    "#for each paper at every index and reduce action to find minimum number, here I used a function find_min()\n",
    "min_no_ratings = paper_rates_rdd.reduce(lambda x,y: find_min(x,y)) \n",
    "print(\"Minimum number of rating an item has received is:\",min_no_ratings)\n",
    "# Exercise 1.4.g)\n",
    "#using the rdd we created earlier which contains number of ratings \n",
    "#for each paper at every index and reduce action to find minimum number, here I used a function find_min()\n",
    "max_no_ratings = paper_rates_rdd.reduce(lambda x,y: find_max(x,y)) \n",
    "print(\"Maximum number of rating an item has received is:\",max_no_ratings)\n",
    "# Exercise 1.4.h)\n",
    "print(\"Average number of ratings of items is:\",ratings_count/number_of_papers)\n",
    "# Exercise 1.4.i)\n",
    "paper_rates_rdd_list = paper_rates_rdd.collect() \n",
    "# now that we have a RDD which each item of it is the number of papers each user liked,\n",
    "# we can easily use python statistics library to find the standard deviation of them.\n",
    "print(statistics.stdev(paper_rates_rdd_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6af511-750b-4836-80fa-8e14d1c634d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exercise 1.5\n",
    "spark = SparkSession.builder.appName(\"DataFrame\").getOrCreate()\n",
    "# Defining an appropriate schema for dataset\n",
    "userSchema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),        \n",
    "    StructField(\"liked_papers\", StringType(), True)\n",
    "])\n",
    "# read from file\n",
    "user_paper_df = spark.read.load(\"/home/jovyan/Data/users_libraries.txt\", format=\"csv\", header=\"false\", sep=';', schema=userSchema)\n",
    "user_paper_df.show()\n",
    "#----------------------------------------------------------------------------\n",
    "# Defining an appropriate schema for dataset\n",
    "paperSchema = StructType([\n",
    "    StructField(\"paper_id\", StringType(), False),        \n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"journal\", StringType(), True),\n",
    "    StructField(\"book_title\", StringType(), True),\n",
    "    StructField(\"series\", StringType(), True),\n",
    "    StructField(\"publisher\", StringType(), True),\n",
    "    StructField(\"pages\", StringType(), True),\n",
    "    StructField(\"volume\", StringType(), True),\n",
    "    StructField(\"number\", StringType(), True),\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"month\", StringType(), True),\n",
    "    StructField(\"postedat\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"abstract\", StringType(), True),\n",
    "])\n",
    "# read from file\n",
    "papers_df = spark.read.load(\"/home/jovyan/Data/papers.csv\", format=\"csv\", header=\"false\", sep=',', schema=paperSchema)\n",
    "papers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e787b6a-9b25-4930-a401-d0687a313686",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exercise 1.6 first part. task 1.3 for data frames\n",
    "# first let's select desired columns and get rid of others\n",
    "paper_content_df = papers_df.select(col(\"paper_id\"),col(\"title\"),col(\"abstract\"))\n",
    "# contructing a df containing user_ids and liked papers, seperated by \",\"\n",
    "df = user_paper_df.select(\"user_id\",PySparkFunctions.split(\"liked_papers\", \",\").alias(\"papers\"))\n",
    "# using explode to flatten the dataframe\n",
    "# in other words; it assings each paper id to one user_id\n",
    "paperid_userid = df.select(explode(\"papers\").alias(\"paper\"),\"user_id\")\n",
    "# contructing a df containing paper_ids and its content which contains title+abstarct\n",
    "paperid_content=paper_content_df.select(\"paper_id\",concat(\"title\",\"abstract\").alias(\"content\"))\n",
    "# now we can join (paperid,userid) and (paperid_content)\n",
    "paperid_userid_content= paperid_userid.join(paperid_content,paperid_userid.paper ==  paperid_content.paper_id,\"inner\")\n",
    "# selecting desired columns user_id and content and spliting the strings in content by seperators\n",
    "userid_words = paperid_userid_content.select(\"user_id\",PySparkFunctions.split(\"content\", ' |;|\"|\\\\{|\\\\}').alias(\"words\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b253f998-e58d-48bd-8041-c447871dedc9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# using groupBy to combine all words which are in papers a user liked together using aggregate function and collect_list\n",
    "userid_allwords = userid_words.groupBy('user_id').agg(PySparkFunctions.collect_list(\"words\").alias(\"words\"))\n",
    "# defining a function which first flattens the list of words related to user\n",
    "# and then find the top ten frequent ones\n",
    "def flaten_find_most_frequent(z):\n",
    "    flatten_list = list(chain.from_iterable(z))\n",
    "    return most_frequent(flatten_list,stopwords)\n",
    "# constructing a udf function\n",
    "convertUDF = udf(lambda z: flaten_find_most_frequent(z),StringType())\n",
    "# applying the udf function on words column\n",
    "userid_topten = userid_allwords.select(col(\"user_id\"),convertUDF(col(\"words\")).alias(\"top_ten_words\"))\n",
    "userid_topten.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e234e4bd-7bc9-4933-84de-ecc37393ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1.6 first part. task 1.4 for dataframes\n",
    "# a)\n",
    "print(\"Number of distinct users is:\", user_paper_df.count()) \n",
    "print(\"Number of distinct papers is:\",papers_df.count()) \n",
    "print(\"Number of ratings is:\",paperid_userid.count())\n",
    "# b)\n",
    "# defining a udf to return number of liked papers for each user\n",
    "convertUDF = udf(lambda x: len(x),IntegerType())\n",
    "# applying udf to df which was (user_ids and liked papers, seperated by \",\")\n",
    "user_ratedcount = df.select(col(\"user_id\"),convertUDF(col(\"papers\")).alias(\"rated_count\"))\n",
    "# using aggregate function to get min\n",
    "min_ratings = user_ratedcount.agg({\"rated_count\": \"min\"}).collect()[0][0]\n",
    "print(\"Minimum number of rating a user has given is:\",min_ratings)\n",
    "# c) the same for max\n",
    "max_ratings = user_ratedcount.agg({\"rated_count\": \"max\"}).collect()[0][0]\n",
    "print(\"Maximum number of rating a user has given is:\",max_ratings)\n",
    "# d) and the same for average\n",
    "avg_ratings = user_ratedcount.agg({\"rated_count\": \"avg\"}).collect()[0][0]\n",
    "print(\"Average number of rating a user has given is:\",avg_ratings)\n",
    "# e) and the same for standard deviation\n",
    "stddev_ratings = user_ratedcount.agg({\"rated_count\": \"stddev\"}).collect()[0][0]\n",
    "print(\"Standard deviation of number of rating a user has given is:\",stddev_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2e1008-882b-4c98-a8dd-14416dc7ae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f)\n",
    "# collecting users who liked for each paper\n",
    "paperid_users = paperid_userid.groupBy('paper').agg(PySparkFunctions.collect_list(\"user_id\").alias(\"users_who_liked\"))\n",
    "# create a udf to get len of a column\n",
    "convertUDF = udf(lambda x: len(x),IntegerType())\n",
    "# creating df containing paper id and number of users who liked this paper\n",
    "paper_usercount = paperid_users.select(col(\"paper\"),convertUDF(col(\"users_who_liked\")).alias(\"users_count\"))\n",
    "# no we can find min,max.avg and stddev using this new df \n",
    "# using aggregate function to get min\n",
    "min_ratings = paper_usercount.agg({\"users_count\": \"min\"}).collect()[0][0]\n",
    "print(\"Minimum number of ratings an item has received is:\",min_ratings)\n",
    "# c) the same for max\n",
    "max_ratings = paper_usercount.agg({\"users_count\": \"max\"}).collect()[0][0]\n",
    "print(\"Maximum number of ratings an item has received is:\",max_ratings)\n",
    "# d) and the same for average\n",
    "avg_ratings = paper_usercount.agg({\"users_count\": \"avg\"}).collect()[0][0]\n",
    "print(\"Average number of ratings an item has received is:\",avg_ratings)\n",
    "# e) and the same for standard deviation\n",
    "stddev_ratings = paper_usercount.agg({\"users_count\": \"stddev\"}).collect()[0][0]\n",
    "print(\"Standard deviation number of ratings an item has received is:\",stddev_ratings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
